{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Netflix job postings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like some other companies, Netflix posts its job offers at a platform called Lever. In this example, I capture information on these jobs, extracting it from the Lever web pages. In this simple **web scraping** exercise, I use the Python packages Requests and Beautiful Soup.\n",
    "\n",
    "**Netflix job postings** can be found at `jobs.lever.co/netflix`. I call this page the **main page**. It will display, the day you visit it, about 450 postings. The postings can be filtered by city, team and work type. Most of the postings on display are for teams in from the Streaming division.\n",
    "\n",
    "The main page contains, for each available position, basic information about the job, such as the job title, the location and the team, and a link to a page specific for that position, such as `jobs.lever.co/netflix/2d11d912-bfb3-4d9d-bfa1-0ce036214284`. I call that specific page the **individual page**. The individual page presents a description of the company and the role of the new employee."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing the source code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HTTP** is a protocol for communication between clients and servers. For instance, a client (such as your browser) sends a **HTTP request** to the server. Then the server returns the response to the client. The response contains status information about the request and, when the request is accepted, the requested content. \n",
    "\n",
    "**GET** is one of the most common HTTP methods. It is used to request data from a specified resource. In the Python package `requests`, the function `get` is an implementation of the HTTP method GET. `requests` comes with the Anaconda distribution, so you can import it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `request` function `get` returns an object of a special type (type `requests.models.Response`). The attribute `text` of this object is a string which, for an ordinary web page, is the HTML source code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_str = requests.get('https://jobs.lever.co/netflix').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `html_str` is a string containing the source code of the Netflix Lever main page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the source code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parse HTML code, learning the tree structure it conveys, I use the function `BeautifulSoup` from the package `bs4` (Beautiful Soup, version 4). I import this function with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BeautifulSoup` transforms the string `html_str` into the \"soup\" object `soup`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I use the method `find_all` to extract from the soup the data on the title, as a list. Every term of the list will be one job title. I will repeat the exercise with the job location and the team for every job. To get extra information that could be found there, I will also extract a list with the links to the individual job pages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a web scraping job, we take advantage of the fact that web pages posting information units in a systematic way have a repetitive structure, in which every unit is contained in a set of HTML elelements with the same names and attributes values. So, by means of `find_all`, we can capture one of features for all the units in one shot, as a list. Let show you how to do this with the job title. \n",
    "\n",
    "The key assumption is that all job titles will be strored in HTML elements with the same name and attribute values, and that this is exclusive for job titles. This is, precisely, what allows Lever to update the pages in a programmatic way with the information supplied by Netflix.\n",
    "\n",
    "To use `find_all`, we need to know the name of the tag and, probably, some of the attributes. How can find this? There are many ways, and every web scraper has his/her own cookbook. The simplest approach is based on browser tools. First, I count the number of times that *APPLY* appears on the page. This is 455 (you will probably get a different number when you visit the page). So I know the number of job titles that I have to capture.\n",
    "\n",
    "Next, I use the *Inspect* tool of the browser. Right-click on the first job title, opening a contextual menu, and select *Inspect*. This will open a window showing a view of the source code in which the element containing that job title is highlighted. The element is:\n",
    "\n",
    "`<h5 data-qa=\"posting-name\">Compositing Supervisor - Wendell & Wild</h5>`\n",
    "\n",
    "So, I try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = soup.find_all('h5', {'data-qa': 'posting-name'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is right, I must have a list with 455 elements. Indeed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be sure, I explore the head and the tail of this list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h5 data-qa=\"posting-name\">Compositing Supervisor - Wendell &amp; Wild</h5>,\n",
       " <h5 data-qa=\"posting-name\">Lead Compositor - Wendell &amp; Wild</h5>,\n",
       " <h5 data-qa=\"posting-name\">Lead Houdini FX Artist  - Wendell &amp; Wild</h5>,\n",
       " <h5 data-qa=\"posting-name\">Production Pipeline TD</h5>,\n",
       " <h5 data-qa=\"posting-name\">VFX Supervisor</h5>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h5 data-qa=\"posting-name\">Manager, CREWS Technology Program</h5>,\n",
       " <h5 data-qa=\"posting-name\">Manager, Space Planning - EMEA</h5>,\n",
       " <h5 data-qa=\"posting-name\">Manager, Space Planning - UCAN</h5>,\n",
       " <h5 data-qa=\"posting-name\">Workplace Manager - Stockholm</h5>,\n",
       " <h5 data-qa=\"posting-name\">Workplace Operations Co-Ordinator - MAC, London</h5>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*. `h1`, `h2`, etc tags are used for headers. They don't have a `class` attribute because their style is unique, specified in a `style` element within the `head` part of the source code. Frequently you don't need the attribute value to capture these elements. In this case, `soup.find_all('h5')` would have given you the same result.\n",
    "\n",
    "To extract the text from every element, I use a `for` loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Compositing Supervisor - Wendell & Wild',\n",
       " 'Lead Compositor - Wendell & Wild',\n",
       " 'Lead Houdini FX Artist  - Wendell & Wild',\n",
       " 'Production Pipeline TD',\n",
       " 'VFX Supervisor']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job = [j.text for j in job]\n",
    "job[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the job location, which is found, following the same approach as for the job title, as the text within a `span` tag with `class=\"sort-by-location posting-category small-category-label\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oregon',\n",
       " 'Oregon',\n",
       " 'Oregon',\n",
       " 'Los Angeles, California',\n",
       " 'Los Angeles, California']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location = soup.find_all('span', 'sort-by-location posting-category small-category-label')\n",
    "location = [l.text for l in location]\n",
    "location[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The team is found in a `span` tag with `class=\"sort-by-team posting-category small-category-label\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Animation – Animation',\n",
       " 'Animation – Animation',\n",
       " 'Animation – Animation',\n",
       " 'Animation – Animation',\n",
       " 'Animation – Animation']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team = soup.find_all('span', 'sort-by-team posting-category small-category-label')\n",
    "team = [t.text for t in team]\n",
    "team[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The team comes in two parts: (a) a division, such *Animation* or *Gaming*, and (b) a department, such as *Art* or *Production Management*. It might be interesting to split it in these two parts, which are separated by a symbol which looks like a hyphen but it is a bit longer. It is the **en dash** (see `jkorpela.fi/dashes.html` if you are curious about this). You can copypaste it in a Jupyter interface, or use the Unicode representation \\u2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Animation', 'Animation'],\n",
       " ['Animation', 'Animation'],\n",
       " ['Animation', 'Animation'],\n",
       " ['Animation', 'Animation'],\n",
       " ['Animation', 'Animation']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team = [t.split(' – ') for t in team]\n",
    "team[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the split has been performed, I name the two parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Animation', 'Animation', 'Animation', 'Animation', 'Animation']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "division = [t[0] for t in team]\n",
    "division[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Animation', 'Animation', 'Animation', 'Animation', 'Animation']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept = [t[1] for t in team]\n",
    "dept[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the *Inspect* tool, one can learn that there is a rectangular area of the page associated to every job posting. For the first posting, that area corresponds to a HTML element with a `div` tag, with `class=\"posting\"` and `data-qa-posting-id=\"bec6d4e4-47ac-4ec0-b2b2-34ad1845de11\"`. The `posting-id` value is specific for that posting, and it works as an ID for the job. The link is formed by pasting the ID after the fixed part `https://jobs.lever.co/netflix/`.\n",
    "\n",
    "So, one way of getting the links is by capturing the ID as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = soup.find_all('div', 'posting')\n",
    "id = [i['data-qa-posting-id'] for i in id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, since we know that every link has to appear as the value of an `href` attribute at an `a` tag, we can search directly for these tags. Indeed, just below the `h5` tag enclosing the job title, the link appears twice, in two `a` tags. The second one has `class=\"posting-title\"`. So, the links can be captured as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://jobs.lever.co/netflix/bec6d4e4-47ac-4ec0-b2b2-34ad1845de11',\n",
       " 'https://jobs.lever.co/netflix/78b120a0-1f92-4f74-b894-716fa7fb83fc',\n",
       " 'https://jobs.lever.co/netflix/3f9031e5-e350-483c-be7e-508236645b8c',\n",
       " 'https://jobs.lever.co/netflix/f0615765-1451-42ae-bf76-7d3dfc1de481',\n",
       " 'https://jobs.lever.co/netflix/d98d1d6a-3b80-4a6d-b0f0-e1648bbd6034']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = soup.find_all('a', 'posting-title')\n",
    "link = [l['href'] for l in link]\n",
    "link[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, leaving aside `team` and `id`, I have the five lists, `job`, `location`, `division`, `dept` and `link`. I can pack them as the columns of a Pandas data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 455 entries, 0 to 454\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   job       455 non-null    object\n",
      " 1   location  455 non-null    object\n",
      " 2   division  455 non-null    object\n",
      " 3   dept      455 non-null    object\n",
      " 4   link      455 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 17.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'job': job, 'location': location, 'division': division, 'dept': dept, 'link': link})\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job</th>\n",
       "      <th>location</th>\n",
       "      <th>division</th>\n",
       "      <th>dept</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Compositing Supervisor - Wendell &amp; Wild</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Animation</td>\n",
       "      <td>Animation</td>\n",
       "      <td>https://jobs.lever.co/netflix/bec6d4e4-47ac-4e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lead Compositor - Wendell &amp; Wild</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Animation</td>\n",
       "      <td>Animation</td>\n",
       "      <td>https://jobs.lever.co/netflix/78b120a0-1f92-4f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lead Houdini FX Artist  - Wendell &amp; Wild</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Animation</td>\n",
       "      <td>Animation</td>\n",
       "      <td>https://jobs.lever.co/netflix/3f9031e5-e350-48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Production Pipeline TD</td>\n",
       "      <td>Los Angeles, California</td>\n",
       "      <td>Animation</td>\n",
       "      <td>Animation</td>\n",
       "      <td>https://jobs.lever.co/netflix/f0615765-1451-42...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VFX Supervisor</td>\n",
       "      <td>Los Angeles, California</td>\n",
       "      <td>Animation</td>\n",
       "      <td>Animation</td>\n",
       "      <td>https://jobs.lever.co/netflix/d98d1d6a-3b80-4a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        job                 location  \\\n",
       "0   Compositing Supervisor - Wendell & Wild                   Oregon   \n",
       "1          Lead Compositor - Wendell & Wild                   Oregon   \n",
       "2  Lead Houdini FX Artist  - Wendell & Wild                   Oregon   \n",
       "3                    Production Pipeline TD  Los Angeles, California   \n",
       "4                            VFX Supervisor  Los Angeles, California   \n",
       "\n",
       "    division       dept                                               link  \n",
       "0  Animation  Animation  https://jobs.lever.co/netflix/bec6d4e4-47ac-4e...  \n",
       "1  Animation  Animation  https://jobs.lever.co/netflix/78b120a0-1f92-4f...  \n",
       "2  Animation  Animation  https://jobs.lever.co/netflix/3f9031e5-e350-48...  \n",
       "3  Animation  Animation  https://jobs.lever.co/netflix/f0615765-1451-42...  \n",
       "4  Animation  Animation  https://jobs.lever.co/netflix/d98d1d6a-3b80-4a...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting to CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can export the data to a CSV file by means of the function `to_csv` (edit the path of the file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('netflix.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument `index=False` to skip the default of `to_csv`, which is to add a column containing the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In this example, I have used the browser tools to find the elements containing the data. Nevertheless, in most cases you can use the method `find_all`to find them. For instance, if you know that the first title is 'Compositing Supervisor - Wendell & Wild', you can use this information to find the tag. To do this, import the package `re`, and use an appropriate regular expression `expr`, covering any potential tag name, in `soup.find_all(re.compile(expr), text='Compositing Supervisor - Wendell & Wild')`.\n",
    "\n",
    "2. The method suggested above would work for any data that come as the text within a tag. Use it again to find the tag for the location of the first job posting. Since the location and the team are displayed in upper case in the web page and one can trust upper case in web pages, you may have to refine the method, using a regular expression as the text.\n",
    "\n",
    "3. For the team, the method can give you trouble if you use `text='animation'`, but it will work fine with `text='animation – animation'`.\n",
    "\n",
    "4. Find the elements containing the links using `find_all`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
